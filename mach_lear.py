# -*- coding: utf-8 -*-
"""Mach_lear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IOp7LeP7v802O6oVv4chrTagIyvQNAEn
"""

def main():
  #import libraries
  import numpy as np
  from matplotlib import pyplot as plt
  from sklearn.datasets.samples_generator import make_blobs
  from numpy import save
  from sklearn.cluster import DBSCAN
  import seaborn as sns
  #Generate the random data with 4 clusters and save the data points.
  data = generate_save_data()
  #Cluster the randomly generated data using DBscan
  cluster_data_db(X)
  #Cluster the sample data using DBScan
  sample = np.load("/content/sample_data.npy")
  sample_data(sample)
  sample_data_db(sample)

if __name__ == "__main__":
    main()
  #A
# DBSCAN as the name implies Density Based Spatial Clustering of Appliocations with noise is very good at clustering the high density data 
#but when the data becomes similar in density, it may be either clustered as a separate cluster or clustered in a wrong Cluster. In the sample data the points are located quite far as compared to the generated data 
#due to which after DBscan the Sample data points lying far are considered to be different clusters and as a result it creates many clusters of the low-dense points.

#B
#Its not a good choice for creating clusters as it generates more centres around which the data is located as compared to the original data.That makes the statistical analysis of the data difficult.

#C
# To get a better clustering performance for the sample data, I would tune the epsilon factor in DBSCAN algorithm.I will make it higher for eg: eps = 3.0 
#because then it will consider the data-points in a circle of radius 3.0 and this way many points which were far will start aligning in neat clusters.
#Also, i will tune the min_samples parameter in DBscan for-eg. min_samples = 4, that increases the minimum number of points to be considered for the cluster. 

#D
#DBSCAN belongs to the "UNSUPERVISED MACHINE LEARNING". It belongs to this class of ML because the data points are clustered based upon a similar criteria i.e. distance whereas
#in other ML algorithms like Supervised/Reinforcement the motive for data aligning is based upon some statistical/Geometrical features with a pre-defined outcome.

def generate_save_data():
  X, y_true = make_blobs(n_samples=200, centers=4,
                         cluster_std=0.50, random_state=0)
  fig = plt.figure(figsize=(10, 10))

  fg_color = 'white'
  fig.suptitle('Original Data', fontsize=14, fontweight='bold', color = fg_color)
  sns.scatterplot(X[:,0], X[:,1], hue=["cluster-{}".format(x) for x in y_true])
  save("/content/data_points.npy/data.npy",data)
  return X

def cluster_data_db(X):
  clustering = DBSCAN(eps=0.50, min_samples=2).fit(X)
  labels = clustering.labels_
  fig = plt.figure(figsize=(10, 10))
  fg_color = 'white'
  fig.suptitle('DB clustered Data', fontsize=14, fontweight='bold', color = fg_color)
  sns.scatterplot(X[:,0], X[:,1], hue=["cluster-{}".format(x) for x in labels])

def sample_data(sample):
  fig = plt.figure(figsize=(10, 10))
  fg_color = 'white'
  fig.suptitle('Sample Data', fontsize=14, fontweight='bold', color = fg_color)
  sns.scatterplot(sample[:,0], sample[:,1])

def sample_data_db(sample):
  sample_clustering = DBSCAN(eps=0.50, min_samples=2).fit(sample)
  sample_labels = sample_clustering.labels_
  fig = plt.figure(figsize=(10, 10))
  fg_color = 'white'
  fig.suptitle('Sample DB clustered Data', fontsize=14, fontweight='bold', color = fg_color)
  sns.scatterplot(sample[:,0], sample[:,1], hue=["cluster-{}".format(x) for x in sample_labels])





